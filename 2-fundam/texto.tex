\chapter{Fundamentação Teórica}
\label{chap:Fundamentação Teórica}

Este capítulo apresenta os conceitos teóricos fundamentais para o desenvolvimento deste trabalho. 
São abordados tópicos relacionados ao aprendizado de máquina, processamento de linguagem natural, 
redes neurais recorrentes e Long Short-Term Memory (LSTM), Bidirectional Encoder Representations 
from Transformers (BERT), a base de dados utilizada para o treinamento e avaliação dos modelos 
propostos, bem como as métricas de avaliação empregadas. Esses conceitos são essenciais para 
compreender as técnicas e metodologias aplicadas na detecção de sarcasmo.

\section{Processamento de Linguagem Natural}

Processamento de Linguagem Natural (PLN) é um campo de pesquisa que tem como objetivo desenvolver 
sistemas e métodos para permitir que computadores compreendam, interpretem e gerem linguagem humana 
de forma automática \cite{caseli23}. É "natural" no sentido de que a linguagem é humana, em oposição a linguagens
de programação ou outras formas de comunicação estruturada.


Este campo de pesquisa divide-se em duas áreas principais \cite{caseli23}: a interpretação da linguagem 
natural (ILN) e a geração de linguagem natural (GLN). A ILN concentra-se na compreensão, ou análise, e 
interpretação da linguagem humana, permitindo que os computadores extraíam significado e informações 
úteis a partir de textos ou fala. Uma aplicação comum da ILN é um \textit{chatbot}, que pode entender 
perguntas feitas em linguagem natural e fornecer respostas relevantes. Já a GLN foca na criação de 
textos ou fala em linguagem humana de forma automática. Isso envolve a geração de textos coerentes e 
contextualmente apropriados, como resumos automáticos de documentos, geração de relatórios ou, usando 
novamente o exemplo do \textit{chatbot}, a geração de respostas em linguagem natural para as perguntas 
dos usuários.


Neste trabalho, o foco está na ILN, especificamente na detecção de sarcasmo em títulos de notícias. 
Essa tarefa envolve a análise e interpretação de textos para identificar expressões sarcásticas, 
o que requer uma compreensão entre as palavras, o contexto e as nuances da linguagem utilizada. 
Para tal, modelos de aprendizado de máquina são treinados para capturar essas relações e
realizar a classificação dos títulos como sarcásticos ou não sarcásticos. Modelos podem ser 
treinados do zero, ou podem ser utilizados modelos pré-treinados, que já possuem um conhecimento
prévio sobre a linguagem, e são ajustados para a tarefa específica de detecção de sarcasmo. Treinar um 
modelo é atualizá-lo e uma das formas de fazer isso é o treinamento continuado.


O treinamento continuado é treinar o modelo pré-treinado em uma nova base de dados, que é específica 
para a tarefa desejada que é diferente da base de dados utilizada no pré-treinamento, porém mantendo
a mesma tarefa geral. A detecção de sarcasmo em títulos de notícias é uma tarefa específica dentro do
campo mais amplo do processamento de linguagem natural: a análise de sentimentos. Portanto, o 
treinamento continuado é uma abordagem adequada para adaptar modelos pré-treinados para essa tarefa 
específica. Existem dois tipos de treinamento continuado: O treinamento continuado com foco na 
tarefa (TAPT - Task Adaptive Pre-Training) e o treinamento continuado com foco na adaptação ao domínio
(DAPT - Domain Adaptive Pre-Training) \cite{caseli23}. 


O TAPT envolve a adaptação de um modelo pré-treinado para uma tarefa específica, utilizando uma base de dados
não rotulada relacionada à tarefa. No caso da detecção de sarcasmo em títulos de notícias, o TAPT poderia
ser realizado utilizando uma grande coleção de títulos de notícias, sem rótulos, para ajustar o modelo 
pré-treinado. Isso permitiria que o modelo aprendesse as características específicas dos títulos de notícias, 
melhorando sua capacidade de detectar sarcasmo nessa tarefa específica. Já o DAPT está preocupado com o 
domínio. Neste caso, o modelo é treinado por mais algum tempo em um domínio específico. Detecção de sarcasmo
em títulos de notícias é uma tarefa que envolve o domínio das notícias, portanto, a abordagem 
utilizada neste trabalho foi o DAPT, onde o modelo pré-treinado é ajustado utilizando uma base de dados
de títulos de notícias, tanto sarcásticos quanto não sarcásticos. 

Treinar um modelo é uma tarefa de aprendizado de máquina, tópico este que será abordado na próxima seção.
Nas seções seguintes, serão apresentados os modelos de aprendizado de máquina utilizados neste trabalho,
Redes Neurais Recorrentes (RNNs) e Long Short-Term Memory (LSTM), e o modelo BERT.


\section{Aprendizado de Máquina}

De acordo com \cite{mitchell97}:
\begin{quote}
    "Um programa de computador aprende com a experiência E em relação a uma tarefa T e uma medida de
    desempenho P, se seu desempenho em T, medido por P, melhora com a experiência E."
\end{quote}
Isto significa que um sistema de aprendizado de máquina é capaz de melhorar seu desempenho em uma
tarefa específica à medida que é exposto a mais dados ou experiências relacionadas a essa tarefa. 

O aprendizado de máquina pode ser dividido em três categorias principais \cite{alpaydin20}, aprendizado
supervisionado, aprendizado não supervisionado e aprendizado por reforço. No aprendizado supervisionado,
o modelo é treinado com um conjunto de dados rotulado, onde cada exemplo de entrada está associado a  
uma saída desejada. O objetivo do modelo é aprender a mapear entradas para saídas corretas, de forma
que possa fazer previsões precisas em novos dados. Já no aprendizado não supervisionado, o modelo é 
treinado com um conjunto de dados não rotulado, onde o objetivo é descobrir padrões ou estruturas 
subjacentes nos dados. Isso pode incluir tarefas como agrupamento (clustering) ou redução de dimensionalidade. 
Por fim, no aprendizado por reforço, o modelo aprende a tomar decisões sequenciais em um ambiente, 
recebendo recompensas ou punições com base em suas ações. O objetivo é aprender uma política que 
maximize a recompensa acumulada ao longo do tempo.


O Aprendizado Profundo \cite{goodfellow16} é uma subárea do aprendizado de máquina que se 
concentra no uso de redes neurais profundas, ou seja, redes neurais com múltiplas camadas ocultas. Essas 
redes são capazes de aprender representações hierárquicas dos dados, permitindo que o modelo capture
padrões complexos e relações não lineares. O Aprendizado Profundo tem sido particularmente bem-sucedido em 
tarefas como reconhecimento de imagem, processamento de linguagem natural e jogos, onde grandes
quantidades de dados e poder computacional estão disponíveis. 


Neste trabalho, o foco está no aprendizado supervisionado, onde modelos de Aprendizado Profundo são treinados
para detectar sarcasmo em títulos de notícias. Os modelos utilizados incluem redes neurais recorrentes
(LSTM) e o modelo BERT, ambos capazes de capturar o contexto e as nuances presentes na linguagem natural.
 


\section{Redes Neurais Recorrentes e LSTM}

Redes Neurais Recorrentes (RNNs) são uma classe de redes neurais projetadas para lidar com dados 
sequenciais, como texto ou séries temporais. Ela funciona mantendo um estado interno que captura 
informações sobre elementos anteriores na sequência, permitindo que a rede aprenda dependências 
de longo prazo \cite{hochreiter97}. No entanto, as RNNs tradicionais enfrentam desafios como o 
problema do desvanecimento do gradiente, que dificulta o aprendizado de dependências de longo prazo.


Para superar essas limitações, foram desenvolvidas variantes das RNNs, como as Long Short-Term
Memory (LSTM) \cite{hochreiter97}. As LSTMs introduzem uma arquitetura de célula especial que inclui 
portas de entrada, esquecimento e saída, permitindo que a rede controle o fluxo de informações e 
mantenha informações relevantes por períodos mais longos. Isso é particularmente útil para tarefas de 
processamento de linguagem natural, onde o contexto e a ordem das palavras são importantes para a compreensão do 
significado. 


O modelo LSTM funciona da seguinte maneira: a porta de entrada decide quais informações da entrada 
atual devem ser armazenadas na célula de memória, a porta de esquecimento determina quais informações 
devem ser descartadas da célula de memória, e a porta de saída controla quais informações da célula de 
memória devem ser usadas para gerar a saída da rede. Essa arquitetura permite que as LSTMs aprendam 
dependências de longo prazo de forma mais eficaz do que as RNNs tradicionais. Isso porque as LSTMs
são capazes de manter informações relevantes na célula de memória por períodos mais longos, enquanto
as RNNs tradicionais tendem a esquecer informações importantes devido ao problema do desvanecimento do
gradiente.


Visto que este trabalho visa detectar sarcasmo em títulos de notícias, é de suma importância capturar 
o contexto e as nuances presentes na linguagem e compreender como as palavras se relacionam entre si. 
Logo, para melhor capturar essas relações, foi utilizado o BiLSTM (Bidirectional LSTM). 
O BiLSTM é uma Bidirectional Recurrent Neural Network (BRNN) \cite{schuster97}, isto é, uma rede 
neural recorrente que processa a sequência de dados em ambas as direções, para frente e para trás. 
Isso permite que o modelo tenha acesso a informações contextuais tanto do passado quanto do futuro, 
melhorando sua capacidade de compreender o significado das palavras em um determinado contexto.


O exemplo a seguir ilustra o funcionamento do BiLSTM. Considere a frase "Ele não gostou do filme 
porque era muito longo". Ao processar essa frase, o LSTM unidirecional pode ter dificuldade em entender 
o significado da palavra "porque", pois ela depende do contexto fornecido pelas palavras que a seguem. 
No entanto, o BiLSTM pode capturar essa dependência, pois processa a frase em ambas as direções, permitindo 
que o modelo compreenda o significado completo da frase. Como este trabalho envolve a detecção de sarcasmo 
em títulos de notícias, o uso do BiLSTM é particularmente vantajoso, pois o sarcasmo muitas vezes depende
do contexto e das nuances presentes na linguagem. Ao utilizar o BiLSTM, o modelo pode capturar melhor essas 
relações e melhorar sua capacidade de detectar sarcasmo.


O modelo BiLSTM foi utilizado em algumas vezes na literatura para a detecção de sarcasmo e apresentou 
resultados promissores. Isso será apresentado no próximo capítulo, onde serão discutidos os trabalhos 
relacionados a este tema.


\section{BERT}

O BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin19} é um modelo de linguagem
baseado na arquitetura Transformer \cite{vaswani17}, que foi pré-treinado em uma grande quantidade de dados 
textuais. Uma arquitetura Transformer é composta por camadas de atenção, que são camadas que permitem 
que o modelo foque em diferentes partes da entrada ao processar uma sequência, e camadas feed-forward,
que são camadas totalmente conectadas que processam as representações intermediárias geradas pelas camadas 
de atenção. Essa arquitetura permite que o modelo capture o contexto bidirecional das palavras, 
o que o torna muito eficaz para tarefas de processamento de linguagem natural. O BERT consiste em 
camadas empilhadas de codificadores Transformer (Transformer Encoders), que são responsáveis por
processar a entrada e gerar representações contextuais das palavras. 


O BERT é um modelo baseado em Transformers pré-treinado em uma grande quantidade de dados textuais utilizando duas 
tarefas principais: Masked Language Modeling (MLM) e Next Sentence Prediction (NSP). No MLM, o modelo é 
treinado para prever palavras mascaradas em uma sequência de texto, isto é, algumas palavras são substituídas 
por um token especial [MASK], e o modelo deve prever quais palavras foram mascaradas com base no contexto fornecido 
pelas palavras restantes. No NSP, o modelo é treinado para prever se uma sentença B segue uma sentença A na 
sequência original do texto. Essas tarefas permitem que o BERT aprenda representações ricas das palavras e do 
contexto em que elas aparecem, o que o torna muito eficaz para umampla variedade de tarefas de processamento de 
linguagem natural.


O BERT utilizado neste trabalho é o BERTimbau \cite{Souza20}, que é uma versão do BERT treinada em 
português. O BERTimbau foi treinado usando dados de BrWaC , uma grande coleção de textos em português
da web. Como resultado, o BERTimbau é capaz de capturar as nuances e particularidades da língua portuguesa,
o que o torna especialmente adequado para tarefas de processamento de linguagem natural nessa língua.


A função base do BERT é gerar representações contextuais das palavras em uma sequência de texto. Essas
representações são vetores densos que capturam o significado e o contexto das palavras, levando
em consideração as palavras que as cercam. Essas representações podem ser utilizadas como entradas
para outros modelos de aprendizado de máquina, como redes neurais, para realizar tarefas específicas
de processamento de linguagem natural, função essa que foi utilizada neste trabalho e descrita 
nos capítulos seguintes.


\section{Métricas de Avaliação}

Métricas de avaliação são essenciais para medir o desempenho dos modelos de aprendizado de máquina. 
Elas fornecem uma maneira objetiva de comparar diferentes modelos e técnicas, permitindo identificar 
quais abordagens são mais eficazes para uma determinada tarefa. Neste trabalho, serão utilizadas as 
seguintes métricas para avaliar o desempenho dos modelos propostos: acurácia, precisão, recall e 
F1-score, sendo está última a métrica principal para comparação entre os modelos.


A acurácia é uma métrica que indica a proporção de previsões corretas em relação ao total de previsões feitas
onde TP é o número de verdadeiros positivos, TN é o número de verdadeiros negativos, FP é o número de falsos positivos e FN é o 
número de falsos negativos. A acurácia é uma métrica útil quando as classes estão balanceadas, mas pode ser enganosa em casos de
classes desbalanceadas. 
\begin{equation}
    \text{Acurácia} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}


A precisão é a proporção de verdadeiros positivos em relação ao total de previsões positivas feitas 
pelo modelo. A precisão é importante quando o custo de falsos positivos é alto. 
\begin{equation}
    \text{Precisão} = \frac{TP}{TP + FP}
\end{equation}


O recall, também conhecido como sensibilidade, é a proporção de verdadeiros positivos em relação ao 
total de casos positivos reais, isto é, em relação ao total de exemplos que realmente pertencem à 
classe positiva, ele é crucial quando o custo de falsos negativos é alto. 
\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
\end{equation}


O F1-score é a média harmônica entre precisão e recall, fornecendo uma métrica balanceada que leva 
em conta ambos. O F1-score é especialmente útil quando há um trade-off, ou seja, quando aumentar a 
precisão pode diminuir o recall e vice-versa. É desejável uma métrica que considere ambos os aspectos.
\begin{equation}
    F1 = 2 \times \frac{\text{Precisão} \times \text{Recall}}{\text{Precisão} + \text{Recall}}
\end{equation}


Esses métricas serão utilizadas para avaliar o desempenho dos modelos propostos na detecção de sarcasmo
em títulos de notícias em português. Os resultados serão apresentados e discutidos no capítulo sobre 
avaliação experimental.

\section{Considerações}

Neste capítulo, foram apresentados os conceitos teóricos fundamentais para o desenvolvimento deste
trabalho. Foram abordados tópicos relacionados ao processamento de linguagem natural, aprendizado de
máquina, redes neurais recorrentes e LSTM, o modelo BERT, bem como as métricas de avaliação utilizadas.
Esses conceitos são essenciais para compreender as técnicas e métodos aplicados na detecção de sarcasmo em títulos de notícias em português, que serão detalhadas nos capítulos seguintes.