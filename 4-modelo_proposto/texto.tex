\chapter{Modelo Proposto}
\label{chap:modelo_proposto}

Neste capítulo, será apresentado o modelo proposto para a detecção de sarcasmo em títulos de notícias 
em português. O modelo combina a arquitetura BiLSTM com o BERT como extrator de características, visando
aproveitar o poder do BERT para capturar o contexto e as nuances da linguagem, enquanto o BiLSTM é responsável
por aprender as relações temporais e sequenciais dos dados.


\section{Ferramentas e Bibliotecas}

Os experimentos realizados neste trabalho foram feitos no Google Colab, que é uma plataforma de 
computação em nuvem que permite a execução de código Python em notebooks Jupyter. O Google Colab 
oferece acesso a GPUs, o que é essencial para o treinamento de modelos de deep learning, como o LSTM 
e o BERT. Além disso, o Google Colab oferece uma série de bibliotecas pré-instaladas, como TensorFlow,
Keras e Transformers, que facilitam o desenvolvimento e a implementação dos modelos. Como ambiente de
desenvolvimento, foi utilizado o Jupyter Notebook, que é uma ferramenta interativa que permite a criação e 
compartilhamento de documentos que contêm código, visualizações e texto explicativo. O Jupyter Notebook é amplamente 
utilizado na comunidade de ciência de dados e machine learning devido à sua facilidade de uso e flexibilidade.
Para a implementação dos modelos LSTM e BERT, foram utilizadas as bibliotecas TensorFlow e Keras.


O TensorFlow é uma biblioteca de código aberto desenvolvida pelo Google para computação numérica e 
machine learning. Ele oferece uma ampla gama de ferramentas e recursos para a construção, treinamento 
e implantação de modelos de machine learning, incluindo suporte para redes neurais profundas e
computação distribuída. O TensorFlow é amplamente utilizado na comunidade de machine learning devido à
sua flexibilidade, escalabilidade e desempenho. Já o Keras é uma biblioteca de alto nível construída sobre o TensorFlow, que 
fornece uma interface simples e intuitiva para a construção e treinamento de modelos de deep learning. O Keras oferece uma série de 
camadas pré-construídas, otimizadores e funções de perda, o que facilita o desenvolvimento de modelos complexos.


Além disso, para a utilização do modelo BERT, foi utilizada a biblioteca Transformers, desenvolvida pela Hugging Face.
A biblioteca Transformers oferece uma ampla gama de modelos pré-treinados baseados na arquitetura Transformer,
incluindo o BERT, GPT e RoBERTa. Ela fornece uma interface simples para o carregamento e utilização desses modelos, bem como 
ferramentas para o fine-tuning e avaliação. A biblioteca Transformers é amplamente utilizada na comunidade de processamento 
de linguagem natural devido à sua facilidade de uso e à qualidade dos modelos pré-treinados disponíveis.
Como otimizador, foi utilizado o Adam (Adaptive Moment Estimation), que é um dos otimizadores mais populares e eficazes para o 
treinamento de redes neurais profundas.


Além disso, foi também utilizado o Pandas e o NumPy. O Pandas é uma biblioteca de código aberto que 
fornece estruturas de dados e ferramentas de análise de dados para a linguagem Python. Ele é amplamente
utilizado na comunidade de ciência de dados devido à sua facilidade de uso e eficiência no manuseio de grandes 
conjuntos de dados. O NumPy é uma biblioteca fundamental para computação científica em Python,
fornecendo suporte para arrays multidimensionais e uma ampla gama de funções matemáticas e estatísticas.
Ele é amplamente utilizado na comunidade de ciência de dados e machine learning devido à sua eficiência
e desempenho em operações numéricas.


A seguir as versões das principais bibliotecas utilizadas neste trabalho:
\begin{itemize}
    \item TensorFlow: 2.12.0
    \item Pandas: 1.5.3
    \item NumPy: 1.24.2
    \item Scikit-learn: 1.2.2
    \item Transformers: 4.30.2
\end{itemize}


\section{Base de Dados}


A base de dados utilizada neste trabalho foi criada por \cite{misra23} e é composta por títulos de 
notícias extraídos do site TheOnion.com \footnote{\url{https://www.theonion.com/}}, conhecido por seu 
conteúdo satírico e sarcástico, e do site Huffington Post \footnote{\url{https://www.huffpost.com/}}, 
que apresenta notícias sérias e informativas. Esta base de dados está disponível publicamente e pode ser 
acessada através do repositório no GitHub \footnote{\url{https://raw.githubusercontent.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection/refs/heads/master/Sarcasm_Headlines_Dataset.json}}.
O propósito do site The Onion é entreter os leitores com notícias fictícias que utilizam o sarcasmo 
como principal recurso humorístico, isto prove um ambiente ideal para coletar exemplos de sarcasmo em 
títulos de notícias.


Estudos anteriores \ref{chap:trabalhos_relacionados}, sobre detecção de sarcasmo utilizavam 
bases de dados compostas por tweets, que são mensagens curtas postadas na plataforma Twitter. No entanto, 
tweets podem conter abreviações, gírias, erros gramaticais e outros elementos que dificultam a análise
linguística. Além disso, muitas dassas bases foram coletadas de forma automática, utilizando hashtags
como \#sarcasm para identificar mensagens sarcásticas, porém isso de nada garante que todas as mensagens
com essa hashtag sejam realmente sarcásticas, ou que todas as mensagens sarcásticas estejam marcadas com essa hashtag.
Por outro lado, títulos de notícias são geralmente mais formais e estruturados, o que facilita a análise
linguística e a detecção de sarcasmo. Portanto, a utilização de títulos de notícias como base de dados
para detecção de sarcasmo pode proporcionar resultados mais precisos e confiáveis.


A base de dados contém 28.503 títulos de notícias, sendo 14.951 títulos sarcásticos do The Onion e 
13.552 títulos não sarcásticos do Huffington Post, ou seja, a base de dados está balanceada entre as
duas classes. Cada título é acompanhado por um rótulo binário, onde 1 indica que o título é sarcástico 
e 0 indica que o título não é sarcástico. A seguir, são apresentados alguns exemplos de títulos
sarcasticos e não sarcásticos presentes na base de dados:
\begin{itemize}
    \item Título sarcástico: "ford develops new suv that runs purely on gasoline";
    \item Título não sarcástico: "what to know regarding current treatments for ebola";
    \item Título sarcástico: "mother comes pretty close to using word 'streaming' correctly";
    \item Título não sarcástico: "5 ways to file your taxes with less stress".
\end{itemize}
Esses exemplos ilustram a diferença entre títulos sarcásticos, que utilizam humor e ironia para transmitir
uma mensagem, e títulos não sarcásticos, que são informativos e diretos.


Este trabalho é focado na detecção de sarcasmo em títulos de notícias em português. Portanto, a base de 
dados original, que está em inglês, foi traduzida para o português utilizando a API do ChatGPT 
\footnote{\url{https://openai.com/blog/chatgpt}}. O modelo escolhido para a tradução foi o GPT-4.1 Mini,
que é capaz de realizar traduções de boa qualidade, entretanto, é importante ressaltar que a tradução 
automática pode introduzir erros ou nuances que não estavam presentes no texto original. Todos os cuidados 
foram tomados para garantir que a tradução mantivesse o significado e o contexto dos títulos originais.
O prompt utilizado para a tradução foi elaborado para solicitar uma tradução literal, sem tentar interpretar 
o sarcasmo, de forma a preservar o sentido original dos títulos. O prompt utilizado foi o seguinte:
\begin{quote}
    "You are a professional translator. Translate the following English headlines into Brazilian Portuguese.
    Rules:
    1. Preserve the original meaning, tone, and style.
    2. If the sentence contains sarcasm, irony, exaggeration, or humor, keep it intact in Portuguese.
    3. Do not explain or interpret sarcasm — just translate it naturally.
    4. Keep proper names, places, and organizations unchanged.
    5. Return only the translations, numbered correspondingly."
\end{quote}

Além disso, no momento da criação, o modelo foi configurado com uma temperatura de 0.3, o que 
torna o modelo mais conservador, reduzindo a criatividade na resposta.
\newpage
\begin{footnotesize}
\begin{verbatim}
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", 
                   "content": "You are a professional translator."},
                  {"role": "user", "content": prompt}],
        temperature=0.3
    )
\end{verbatim}
\end{footnotesize}
%Paragrafo para introduzir a nuvem de palavras, um gráfico de número de palavras por título e 
%Um gráfico de número de caracteres por título.

Uma análise inicial da base de dados traduzida revelou que os títulos possuem em média 10 a 12 palavras,
com alguns títulos mais longos chegando a ter até 25 palavras. Em termos de caracteres, os títulos tem em 
média entre 50 a 70 caracteres, com alguns títulos mais longos chegando a ter até 150 caracteres. A seguir, são apresentados
dois gráficos que ilustram a distribuição do número de palavras e caracteres por título na base de dados traduzida.

% exemplo de inserção de figura
\begin{figure}[!htb]
\centering
\includegraphics[width=12cm]{4-modelo_proposto/figuras/n_palavras.png}
\caption{Distribuição do número de palavras por título na base de dados traduzida.}
\label{fig:num_words_per_title}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{4-modelo_proposto/figuras/n_caracteres.png}
    \caption{Distribuição do número de caracteres por título na base de dados traduzida.}
    \label{fig:num_chars_per_title}
\end{figure}
Saber o número médio de palavras e caracteres por título é importante para o pré-processamento dos dados
e para a configuração dos modelos de deep learning, como o BiLSTM e o BERT-BiLSTM, que serão utilizados 
neste trabalho. Isso ajuda a determinar o tamanho máximo das sequências de entrada e a escolher as técnicas adequadas.


A base de dados utilizada neste trabalho é composta por títulos de notícias sarcásticas e não sarcásticas, A base de dados então dividida em conjuntos de
treino, validação e teste, com proporções de 70\%, 15\% e 15\%, respectivamente. Essa divisão permite
avaliar o desempenho do modelo de forma adequada, garantindo que o modelo seja treinado em uma parte dos dados
e avaliado em dados não vistos durante o treinamento. Para a divisão dos dados, foi utilizado a função
\texttt{train\_test\_split} do Scikit-learn. A divisão foi feita de forma estratificada, garantindo que a proporção
de títulos sarcásticos e não sarcásticos seja mantida em cada conjunto, além de definir um valor fixo para o parâmetro
\texttt{random\_state}, garantindo a reprodutibilidade dos resultados.


\section{Modelo Base}


Após a preparação dos dados, foi implementado o modelo BiLSTM. Primeiramente, os dados foram divididos 
em conjuntos de treino, validação e teste, conforme descrito anteriormente. Para avaliar o desempenho do 
modelo, a base de teste foi utilizada e os resultados são apresentados no Capítulo \ref{chap:Resultados}.
O modelo BiLSTM foi implementado utilizando a API Sequencial do Keras. A arquitetura do modelo é composta
por uma camada de embedding, seguida por uma camada BiLSTM com 128 unidades, seguida de uma operação
de Global Max Pooling, seguidas por 2 camadas densas com funções de ativação ReLU, intercaladas com
camadas de dropout para evitar overfitting. Finalmente, a camada de saída é uma camada densa com uma 
unidade e função de ativação sigmóide, que produz uma saída entre 0 e 1, indicando a probabilidade da 
manchete ser sarcástica. 


A operação de Global Max Pooling é utilizada para reduzir a dimensionalidade dos dados, condensando as informações 
mais relevantes em um vetor de tamanho fixo. Essa operação seleciona o valor máximo ao longo de cada dimensão, 
capturando as características mais fortes presentes na sequência de saída da camada BiLSTM. As camadas 
densas subsequentes interpretam e refinam essas características, preparando-as para a classificação final. 
A função de ativação ReLU (Rectified Linear Unit) é amplamente utilizada em redes neurais devido à sua
capacidade de introduzir não linearidades no modelo, permitindo que ele aprenda padrões complexos nos dados. 
As camadas de dropout são uma técnica de regularização que ajuda a prevenir o overfitting, desativando 
aleatoriamente uma fração dos neurônios durante o treinamento, o que força o modelo a aprender representações 
mais robustas. Por fim, a camada de saída com função de ativação sigmóide é adequada para problemas de 
classificação binária, produzindo uma probabilidade que pode ser interpretada como a confiança do modelo 
na classificação do título como sarcástico ou não sarcástico. O diagrama da arquitetura do modelo 
BiLSTM é apresentado na Figura \ref{fig:bilstm_model}. 


A função de perda utilizada foi a binary crossentropy, adequada para problemas de classificação binária. 
Essa função mede a diferença entre as distribuições de probabilidade previstas pelo modelo e as 
distribuições reais dos rótulos, penalizando previsões incorretas de forma mais severa. A métrica utilizada para
avaliar o desempenho do modelo durante o treinamento foi a acurácia (accuracy), que indica a proporção de previsões 
corretas feitas pelo modelo em relação ao total de previsões. O otimizador utilizado foi o Adam, com uma 
taxa de aprendizado de 0.00001. O Adam é um dos otimizadores, algoritmo que ajusta os pesos do modelo 
durante o treinamento, com o objetivo de minimizar a função de perda. É um dos mais populares e eficazes para o 
treinamento de redes neurais profundas.  


O modelo foi treinado por um máximo de 25 épocas, com tamanho de lote (batch size) de 100. Durante o 
treinamento, foi utilizado o Early Stopping, que monitora a métrica de validação (val\_loss) e 
interrompe o treinamento se a métrica não melhorar por 3 épocas consecutivas, prevenindo overfitting. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{4-modelo_proposto/figuras/modelo_base.png}
    \caption{Arquitetura do modelo BiLSTM utilizado como baseline.}
    \label{fig:bilstm_model}
\end{figure}

\section{Modelo Proposto: BERT-BiLSTM}


Além do modelo BiLSTM, foi implementado o modelo BERT-BiLSTM, que utiliza o modelo pré-treinado BERT como 
camada de embedding. Como descrito anteriormente, o BERT é um modelo de linguagem baseado na arquitetura 
Transformer, que foi pré-treinado em uma grande quantidade de dados textuais. O BERT é capaz de capturar o 
contexto bidirecional das palavras, o que o torna muito eficaz para tarefas de processamento de linguagem 
natural. 


Primeiramente, os títulos foram tokenizados utilizando o tokenizador do BERTimbau, que converte as palavras
em tokens compatíveis com o modelo. Esses tokens foram então alimentados no modelo, que gerou representações 
vetoriais para cada título. Essas representações foram então utilizadas como entradas para o modelo BiLSTM, 
que realizou a classificação dos títulos em sarcásticos ou não sarcásticos. A arquitetura do modelo BERT-BiLSTM 
é semelhante à do modelo BiLSTM, com a diferença de que a camada de embedding é substituída pelo modelo BERT 
pré-treinado. A função de perda, otimizador, tamanho de lote e estratégia de Early Stopping utilizados foram os mesmos 
do modelo BiLSTM. 


Espera-se que a incorporação do BERT como camada de embedding traga melhorias no desempenho do modelo, 
especialmente na captura do contexto e das nuances da linguagem presentes nos títulos de notícias.


\section{Considerações}


Neste capítulo, foram apresentados o modelo proposto para a detecção de sarcasmo em títulos de notícias
em português, bem como as ferramentas e bibliotecas utilizadas para a implementação dos modelos. Foi
descrita a base de dados utilizada, incluindo o processo de tradução dos títulos para o português.
Além disso, foram detalhados o modelo base BiLSTM e o modelo proposto BERT-BiLSTM, incluindo suas arquiteturas
e configurações de treinamento.


O modelo BiLSTM serve como uma linha de base para comparação com o modelo proposto, permitindo avaliar
o impacto da incorporação do BERT como camada de embedding. Está arquitetura visa aproveitar o poder do 
BERT para capturar o contexto e as nuances da linguagem, enquanto o BiLSTM é responsável por aprender 
as relações temporais e sequenciais dos dados. Está mesma arquitetura foi utilizada em trabalhos 
anteriores - capítulo \ref{chap:trabalhos_relacionados} - e apresentou bons resultados na detecção de sarcasmo 
em textos. Portanto, espera-se que o modelo proposto apresente um desempenho superior ao modelo base, 
especialmente na captura do contexto e das nuances da linguagem presentes nos títulos de notícias.


No próximo capítulo, serão apresentados os experimentos realizados para avaliar o desempenho do
modelo proposto, bem como uma comparação com outros modelos existentes na literatura.
